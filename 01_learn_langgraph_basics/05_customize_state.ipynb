{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fda91568",
   "metadata": {},
   "source": [
    "# 5. Customize state\n",
    "\n",
    "In this tutorial, you will add additional fields to the state to define complex behavior without relying on the message list. The chatbot will use its search tool to find specific information and forward them to a human for review.\n",
    "\n",
    "This tutorial builds on [Add human-in-the-loop controls](/oss/4-human-in-the-loop).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31080f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# Function to securely get OpenAI API key\n",
    "def get_openai_api_key():\n",
    "    \"\"\"Securely prompt user for OpenAI API key\"\"\"\n",
    "    # First try to get from environment variable\n",
    "    api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "    \n",
    "    if not api_key:\n",
    "        print(\"OpenAI API key not found in environment variables.\")\n",
    "        print(\"Please enter your OpenAI API key:\")\n",
    "        print(\"Note: Your input will be hidden for security.\")\n",
    "        api_key = getpass.getpass(\"OpenAI API Key: \")\n",
    "        \n",
    "        if api_key:\n",
    "            # Set it for this session only\n",
    "            os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "            print(\"API key set for this session.\")\n",
    "        else:\n",
    "            raise ValueError(\"OpenAI API key is required to run this chatbot.\")\n",
    "    \n",
    "    return api_key\n",
    "\n",
    "# Get the API key securely\n",
    "api_key = get_openai_api_key()\n",
    "\n",
    "# Initialize the chat model\n",
    "llm = init_chat_model(\"openai:gpt-4.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee44633",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Add keys to the state\n",
    "\n",
    "Update the chatbot to research the birthday of an entity by adding `name` and `birthday` keys to the state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fa0f82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    name: str\n",
    "    birthday: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af3ed7d",
   "metadata": {},
   "source": [
    "Adding this information to the state makes it easily accessible by other graph nodes (like a downstream node that stores or processes the information), as well as the graph's persistence layer.\n",
    "\n",
    "## 2. Update the state inside the tool\n",
    "\n",
    "Now, populate the state keys inside of the `human_assistance` tool. This allows a human to review the information before it is stored in the state. Use [`Command`](/oss/graph-api#using-inside-tools) to issue a state update from inside the tool.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b87e9738",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import ToolMessage\n",
    "from langchain_core.tools import InjectedToolCallId, tool\n",
    "\n",
    "from langgraph.types import Command, interrupt\n",
    "from langchain_tavily import TavilySearch\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "@tool\n",
    "# Note that because we are generating a ToolMessage for a state update, we\n",
    "# generally require the ID of the corresponding tool call. We can use\n",
    "# LangChain's InjectedToolCallId to signal that this argument should not\n",
    "# be revealed to the model in the tool's schema.\n",
    "def human_assistance(\n",
    "    name: str, birthday: str, tool_call_id: Annotated[str, InjectedToolCallId]\n",
    ") -> str:\n",
    "    \"\"\"Request assistance from a human.\"\"\"\n",
    "    human_response = interrupt(\n",
    "        {\n",
    "            \"question\": \"Is this correct?\",\n",
    "            \"name\": name,\n",
    "            \"birthday\": birthday,\n",
    "        },\n",
    "    )\n",
    "    # If the information is correct, update the state as-is.\n",
    "    if human_response.get(\"correct\", \"\").lower().startswith(\"y\"):\n",
    "        verified_name = name\n",
    "        verified_birthday = birthday\n",
    "        response = \"Correct\"\n",
    "    # Otherwise, receive information from the human reviewer.\n",
    "    else:\n",
    "        verified_name = human_response.get(\"name\", name)\n",
    "        verified_birthday = human_response.get(\"birthday\", birthday)\n",
    "        response = f\"Made a correction: {human_response}\"\n",
    "\n",
    "    # This time we explicitly update the state with a ToolMessage inside\n",
    "    # the tool.\n",
    "    state_update = {\n",
    "        \"name\": verified_name,\n",
    "        \"birthday\": verified_birthday,\n",
    "        \"messages\": [ToolMessage(response, tool_call_id=tool_call_id)],\n",
    "    }\n",
    "    # We return a Command object in the tool to update our state.\n",
    "    return Command(update=state_update)\n",
    "\n",
    "tool = TavilySearch(max_results=2)\n",
    "tools = [tool, human_assistance]\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "def chatbot(state: State):\n",
    "    message = llm_with_tools.invoke(state[\"messages\"])\n",
    "    # Because we will be interrupting during tool execution,\n",
    "    # we disable parallel tool calling to avoid repeating any\n",
    "    # tool invocations when we resume.\n",
    "    assert len(message.tool_calls) <= 1\n",
    "    return {\"messages\": [message]}\n",
    "\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "\n",
    "tool_node = ToolNode(tools=tools)\n",
    "graph_builder.add_node(\"tools\", tool_node)\n",
    "\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"chatbot\",\n",
    "    tools_condition,\n",
    ")\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "\n",
    "memory = InMemorySaver()\n",
    "\n",
    "graph = graph_builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a27ada4",
   "metadata": {},
   "source": [
    "he rest of the graph stays the same.\n",
    "\n",
    "## 3. Prompt the chatbot\n",
    "\n",
    "Prompt the chatbot to look up the \"birthday\" of the LangGraph library and direct the chatbot to reach out to the `human_assistance` tool once it has the required information. By setting `name` and `birthday` in the arguments for the tool, you force the chatbot to generate proposals for these fields.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5316604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Can you look up when LangGraph was released? When you have the answer, use the human_assistance tool for review.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  tavily_search (call_TWjezsGgttlFATn2DMlaYQVw)\n",
      " Call ID: call_TWjezsGgttlFATn2DMlaYQVw\n",
      "  Args:\n",
      "    query: LangGraph release date\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: tavily_search\n",
      "\n",
      "{\"query\": \"LangGraph release date\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"url\": \"https://langchain-ai.github.io/langgraph/tutorials/get-started/5-customize-state/\", \"title\": \"5. Customize state - GitHub Pages\", \"content\": \"Prompt the chatbot to look up the \\\"birthday\\\" of the LangGraph library and direct the chatbot to reach out to the `human_assistance` tool once it has the required information. Then, I'll use the human_assistance tool for review.\\\", 'type': 'text'}, {'id': 'toolu_01JoXQPgTVJXiuma8xMVwqAi', 'input': {'query': 'LangGraph release date'}, 'name': 'tavily_search_results_json', 'type': 'tool_use'}] \\\\n\\\\nGiven this information, I'll use the human_assistance tool to review and potentially provide more accurate information about LangGraph's initial release date.\\\", 'type': 'text'}, {'id': 'toolu_01JDQAV7nPqMkHHhNs3j3XoN', 'input': {'name': 'Assistant', 'birthday': '2023-01-01'}, 'name': 'human_assistance', 'type': 'tool_use'}] \\\\n\\\\nGiven this information, I'll use the human_assistance tool to review and potentially provide more accurate information about LangGraph's initial release date.\\\", 'type': 'text'}, {'id': 'toolu_01JDQAV7nPqMkHHhNs3j3XoN', 'input': {'name': 'Assistant', 'birthday': '2023-01-01'}, 'name': 'human_assistance', 'type': 'tool_use'}]\", \"score\": 0.8124067, \"raw_content\": null}, {\"url\": \"https://pypi.org/project/langgraph/\", \"title\": \"langgraph - PyPI\", \"content\": \"langgraph Â· PyPI langgraph 0.6.4 Image 5: LangGraph Logo Install LangGraph: from langgraph.prebuilt import create_react_agent Or, to learn how to build an agent workflow with a customizable architecture, long-term memory, and other complex task handling, see the LangGraph basics tutorials. LangGraph provides low-level supporting infrastructure for _any_ long-running, stateful workflow or agent. While LangGraph can be used standalone, it also integrates seamlessly with any LangChain product, giving developers a full suite of tools for building agents. *   LangGraph Platform â Deploy and scale agents effortlessly with a purpose-built deployment platform for long running, stateful workflows. 0.3.0 yanked Feb 26, 2025 Reason this release was yanked: Missing dependency on langgraph-prebuilt Details for the file `langgraph-0.6.4.tar.gz`. Details for the file `langgraph-0.6.4-py3-none-any.whl`.\", \"score\": 0.709684, \"raw_content\": null}], \"response_time\": 1.16, \"request_id\": \"150747c9-7112-4486-9e0d-74deadf52802\"}\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  human_assistance (call_SwyI8ioAE72ovBqBMoK43HMw)\n",
      " Call ID: call_SwyI8ioAE72ovBqBMoK43HMw\n",
      "  Args:\n",
      "    name: LangGraph\n",
      "    birthday: The exact release date was not explicitly found in the top search results. PyPI lists version history but not the initial release date directly. Do you have the official initial release date for review?\n"
     ]
    }
   ],
   "source": [
    "user_input = (\n",
    "    \"Can you look up when LangGraph was released? \"\n",
    "    \"When you have the answer, use the human_assistance tool for review.\"\n",
    ")\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "events = graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": user_input}]},\n",
    "    config,\n",
    "    stream_mode=\"values\",\n",
    ")\n",
    "for event in events:\n",
    "    if \"messages\" in event:\n",
    "        event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f242f7",
   "metadata": {},
   "source": [
    "We've hit the `interrupt` in the `human_assistance` tool again.\n",
    "\n",
    "## 4. Add human assistance\n",
    "\n",
    "The chatbot failed to identify the correct date, so supply it with information:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d83f9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  human_assistance (call_SwyI8ioAE72ovBqBMoK43HMw)\n",
      " Call ID: call_SwyI8ioAE72ovBqBMoK43HMw\n",
      "  Args:\n",
      "    name: LangGraph\n",
      "    birthday: The exact release date was not explicitly found in the top search results. PyPI lists version history but not the initial release date directly. Do you have the official initial release date for review?\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: human_assistance\n",
      "\n",
      "Made a correction: {'name': 'LangGraph', 'birthday': 'Jan 17, 2024'}\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "LangGraph was officially released on January 17, 2024. This date has been reviewed and corrected through human assistance for accuracy.\n"
     ]
    }
   ],
   "source": [
    "human_command = Command(\n",
    "    resume={\n",
    "        \"name\": \"LangGraph\",\n",
    "        \"birthday\": \"Jan 17, 2024\",\n",
    "    },\n",
    ")\n",
    "\n",
    "events = graph.stream(human_command, config, stream_mode=\"values\")\n",
    "for event in events:\n",
    "    if \"messages\" in event:\n",
    "        event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ef1cdd",
   "metadata": {},
   "source": [
    "Note that these fields are now reflected in the state:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7718ece4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'LangGraph', 'birthday': 'Jan 17, 2024'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "snapshot = graph.get_state(config)\n",
    "\n",
    "{k: v for k, v in snapshot.values.items() if k in (\"name\", \"birthday\")}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5503b03a",
   "metadata": {},
   "source": [
    "This makes them easily accessible to downstream nodes (e.g., a node that further processes or stores the information).\n",
    "\n",
    "## 5. Manually update the state\n",
    "\n",
    "LangGraph gives a high degree of control over the application state. For instance, at any point (including when interrupted), you can manually override a key using `graph.update_state`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d72b9b4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'configurable': {'thread_id': '1',\n",
       "  'checkpoint_ns': '',\n",
       "  'checkpoint_id': '1f08030c-dd02-6664-8006-75ea63266832'}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.update_state(config, {\"name\": \"LangGraph (library)\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae945e1",
   "metadata": {},
   "source": [
    "## 6. View the new value\n",
    "\n",
    "If you call `graph.get_state`, you can see the new value is reflected:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4c3199e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'LangGraph (library)', 'birthday': 'Jan 17, 2024'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snapshot = graph.get_state(config)\n",
    "\n",
    "{k: v for k, v in snapshot.values.items() if k in (\"name\", \"birthday\")}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f752c5c8",
   "metadata": {},
   "source": [
    "Manual state updates will [generate a trace](https://smith.langchain.com/public/7ebb7827-378d-49fe-9f6c-5df0e90086c8/r) in LangSmith. If desired, they can also be used to [control human-in-the-loop workflows](/oss/add-human-in-the-loop). Use of the `interrupt` function is generally recommended instead, as it allows data to be transmitted in a human-in-the-loop interaction independently of state updates.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
